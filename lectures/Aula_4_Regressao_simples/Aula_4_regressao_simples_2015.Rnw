\documentclass{beamer}
\usefonttheme[onlymath]{serif}

\usepackage[portuguese]{babel}
\usepackage{graphicx}
\usepackage{ulem} % Para texto em strikeout
\usepackage{amsmath}
\usepackage{amssymb}

\usetheme{m}

\ifdefined\knitrout 
\renewenvironment{knitrout}{\setlength{\topsep}{0mm}}{}
\else
\fi

\title{Aula 4: Modelos Lineares Gerais I}
\subtitle{Análise Quantitativa de Dados Ambientais}
\author{\textbf{Thiago S. F. Silva} - tsfsilva@rc.unesp.br}
\institute{Programa de Pós Graduação em Geografia - IGCE/UNESP}
\date{\today}

\graphicspath{{C:/Users/thiago/OneDrive/UNESP/Pos_graduacao/Eco/2015/Estatistica_2015/Aulas/Aula_4_Regressao_simples/figuras/}}

\begin{document}

<<opts, echo=FALSE>>=
options(width=80)
library(ggplot2)
plotheme <- theme(axis.line = element_line(colour = "black", size=1), panel.grid.major = element_blank(),panel.grid.minor = element_blank(), panel.border = element_blank(),panel.background = element_blank()) 
@

%===============================================================================%
\begin{frame}[plain] % plain avoids a badbox error from page number in title page
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}
%===============================================================================%

\section{Modelos Lineares Gerais}


%===============================================================================%
\begin{frame}{Modelos Lineares Gerais}

Classe de modelos do tipo $\mathbf{Y = BX + U}$, onde $\mathbf{Y}$ é um vetor de respostas, $\mathbf{B}$ é a matriz desenho (\emph{design matrix}), $\mathbf{X}$ é uma matriz de variáveis explicativas, e $\mathbf{U}$ é uma matriz contendo os erros.

\begin{equation*}
\begin{bmatrix}
  y_1\\
  y_2\\
  y_3\\
  . \\
  . \\
  y_n\\
  \end{bmatrix}
=
\begin{bmatrix}
  1 & x_{11} & \dots & x_{1k} \\
  1 & x_{21} & \dots & x_{2k} \\
  1 & x_{31} & \dots & x_{3k} \\
  . & . & . & . \\
  . & . & . & . \\
  1 & x_{n1} & \dots & x_{nk} \\
  \end{bmatrix}
\times
\begin{bmatrix}
  \beta_0 \\
  \beta_1 \\
  \beta_2 \\
  . \\
  . \\
  \beta_k \\
  \end{bmatrix}
+
\begin{bmatrix}
  \varepsilon_1 \\
  \varepsilon_2 \\
  \varepsilon_3 \\
  . \\
  . \\
  \varepsilon_n \\
  \end{bmatrix}
\end{equation*}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Cuidado!}

A nomenclatura é bastante confusa:

\begin{itemize}
  \item General Linear Models (GLM)
  \item Generalized Linear Models (GLM)
  \item Generalized Linear Mixed Models (GLMM)
  \item Generalized Least Squares (GLS)
\end{itemize}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}
\centering

\only<1-2>{\large{\textbf{Regressão} Linear Simples?}}
\only<3-5>{\large{Regressão \textbf{Linear} Simples?}}
\only<7-8>{\large{Regressão Linear \textbf{Simples?}}}

\vfill

\only<2>{Sir Francis Galton, no século IX, observou que a relação entre alturas de pais e filhos parecia "reverter", ou "regredir" para a média do grupo. A partir dessa observação, Sir Galton desenvolveu uma primeira formulação matemática para a regressão.}

\only<4>{
Os modelos de regressão são formados a partir de combinações lineares de variáveis, através de parâmetros 
\vfill
$Y = \beta _0 +\beta _1 X$ é uma combinação linear de um parâmetro linear $\beta _0$ e um parâmetro linear $\beta _1$ que multiplica $X$ 
\vfill
$Y = \beta _0 +\beta _1 X + \beta _2 X^2$ também é uma combinação linear, de um parâmetro linear $\beta _0$, um parâmetro linear $\beta _1$ que multiplica $X$, e um parâmetro linear $\beta _2$ que multiplica $X^2$
}

\only<5>{

$Y = \beta _0 + e^{\beta _1 X}$ não é uma combinação linear
\vfill
A linearidade se refere aos parâmetros, e não às variáveis
}

\only<6>{

$Y = \beta _0 + e^{\beta _1 X}$ não é uma combinação linear
\vfill
Mas alguns modelos não-lineares podem ser linearizados:
\vfill
$ Ln(Y) = Ln(\beta_0) + \beta_1 X$

$ Z = W + \beta_1 X$

$ Z = Ln(Y)$

$W = Ln(\beta0)$
}

\only<8>{Apenas duas variáveis são usadas, uma dependente ($Y$) e uma independente ($X$)}


\end{frame}
%===============================================================================%



%===============================================================================%
\begin{frame}{Terminologia}
\begin{center}
$Y = \beta _0 + \beta _1 X + \varepsilon$
\end{center}
\vfill
\begin{itemize}
  \item $X$ é a variável \textbf{independente}, também chamada de \textbf{preditor} \pause
  \item $Y$ é a variável \textbf{dependente}, também chamada de \textbf{variável resposta} \pause
  \item $\beta _0$ e $\beta _1$ são os \textbf{parâmetros} ou \textbf{coeficientes} da regressão \pause
  \item $\varepsilon$ é o \textbf{termo de erro}
\end{itemize}  
  
\end{frame}
%===============================================================================%




%===============================================================================%
\begin{frame}{Características de um modelo de regressão}

Os modelos de regressão expressam essencialmente:
\vfill
\begin{itemize}
  \item Uma tendência de $Y$ em variar sistematicamente de acordo com o preditor $X$ \pause
  \item Uma dispersão de pontos ao redor da reta que descreve uma relação estatística  \pause
  \end{itemize}
\vfill
Essas características são expressas através das pressuposições:
\vfill
\begin{itemize}
  \item Existe uma distribuição de probabilidade de $Y$ para cada nível (valor) de $X$ \pause
  \item A média destas distribuições varia sistematicamente com $X$
\end{itemize}  
  
\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Exemplo}
\centering

\only<1>{
\includegraphics[width=1\linewidth]{Fig_[1].jpg}
}

\only<2>{
\includegraphics[width=1\linewidth]{Fig_[2].jpg}
}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Regressão e causalidade}

A existência de uma (cor)relação estatística entre duas variáveis não implica em uma relação real de causalidade ou dependência.
\vfill
Mesmo quando há causalidade, cuidado com a direção da relação: $X$ causa $Y$, ou $Y$ causa $X$?
\vfill
\begin{center}
\includegraphics[width=0.7\linewidth]{correlation.png}

\tiny{\url{http://xkcd.com/552/}}
\end{center}

\end{frame}
%===============================================================================%

\section{Formulação do Modelo de Regressão}


%===============================================================================%
\begin{frame}{Formulação do modelo de regressão}

O modelo de regressão completo pode ser escrito como:
\vfill
\begin{center}
$Y_i=\beta _0 + \beta _1 X_i + \varepsilon _i$
\end{center}
\vfill
``O $i$-ésimo valor de $Y$ é função de um parâmetro constante $\beta _0$, somado ao $i$-ésimo valor de $X$ multiplicado por um parâmetro constante $\beta _1$, somado a um $i$-ésimo valor específico de erro''.

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Formulação do modelo de regressão}

\begin{columns}[c]

\column{0.5\linewidth}
Se $Y_i$ puder ser predito exatamente por $X_i$, então $\varepsilon _i \sim N(0,0)$
\vfill
$Y_i = 2 + 3X_i$
\vfill
<<ezero,echo=FALSE,fig.height=5,fig.width=5,out.width='1\\linewidth'>>=
set.seed(1979)
x <- c(1:10)
y <- 2 + 3*x
plot(x,y,xlim=c(0,10),ylim=c(0,40))
abline(lm(y~x),lwd=2)
@

\column{0.5\linewidth}

\only<1>{
Se $Y_i$ pode ser aproximado por $X_i$, então $\varepsilon_i \sim N(0,\sigma)$
\vfill
$Y_i = 2 + 3X_i + \varepsilon \sim N(0,3)$
\vfill
<<enonzero3,echo=FALSE,fig.height=5,fig.width=5,out.width='1\\linewidth'>>=
y2 <- 2 + 3*x + rnorm(10,0,3)
m2<- lm(y2~x)
y2pred <- predict(m2)
plot(x,y2,xlim=c(0,10),ylim=c(0,40))
abline(lm(y2~x),lwd=2)
segments(x0=x, x1=x,y0=y2pred,y1=y2,col='red',lwd=2)
@
}

\only<2>{
Se $Y_i$ pode ser aproximado por $X_i$, então $\alert{{\varepsilon_i}} \sim N(0,\sigma)$
\vfill
$Y_i = 2 + 3X_i + \varepsilon \sim N(0,6)$
\vfill
<<enonzero10,echo=FALSE,fig.height=5,fig.width=5,out.width='1\\linewidth'>>=
y2 <- 2 + 3*x + rnorm(10,0,6)
m2<- lm(y2~x)
y2pred <- predict(m2)
plot(x,y2,xlim=c(0,10),ylim=c(0,40))
abline(lm(y2~x),lwd=2)
segments(x0=x, x1=x,y0=y2pred,y1=y2,col='red',lwd=2)
@
}

\only<3>{
Se ${\varepsilon}$ não possuir média zero, os erros não "regridem" para a linha de tendência central$
\vfill
$Y_i = 2 + 3X_i + \varepsilon \sim N(6,6)$
\vfill
<<enonzero23,echo=FALSE,fig.height=5,fig.width=5,out.width='1\\linewidth'>>=
y3 <- 2 + 3*x + rnorm(10,6,6)
m3<- lm(y3~x)
y3pred <- predict(m3)
plot(x,y2,xlim=c(0,10),ylim=c(0,40))
abline(lm(y2~x),lwd=2)
segments(x0=x, x1=x,y0=y3pred,y1=y2,col='red',lwd=2)
@
}

\end{columns}

\end{frame}
%===============================================================================%



%===============================================================================%
\begin{frame}{Formulação do modelo de regressão}

Na prática, estamos modelando a \textbf{esperança} de $Y$ para cada nível de $X$:
\vfill
$E[Y_i] = [\beta _0 + \beta _1 X_i + \varepsilon _i]$ \pause
\vfill
$E[Y_i] = \beta _0 + \beta _1 X_i + E[\varepsilon _i]$ \pause
\vfill
Mas nós sabemos que $E[\varepsilon _i] = 0$, então: \pause
\vfill
$E[Y] = \beta _0 + \beta _1 X$

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Formulação do modelo de regressão}

Estas relações implicam nas seguintes propriedades:
\vfill
1) $Y_i$ é a soma de um termo constante ($E[Y]$) e um termo aleatório ($\varepsilon$), então $Y_i$ é uma variável aleatória \pause
\vfill
2) A função de regressão para o modelo é $E[Y] = \beta _0 + \beta _1 X$ \pause
\vfill
3) $Y_i$ desvia do valor determinado pela função de regressão por um erro $\varepsilon _i$

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Formulação do modelo de regressão}

4) Pressupõe-se que os erros $\varepsilon _i$ tem uma variância constante $\sigma ^2$. Se isso é verdade, $Y_i$ tem a mesma variância:

\vfill
$Var[\beta _0 + \beta _1 X_i + \varepsilon _i] = Var[\varepsilon _i] = \sigma ^2$ \pause
\vfill
$Y_i = \beta _0 + \beta _1 X_i + \varepsilon _i$ \pause
\vfill
$Var[Y_i] = Var[\beta _0 + \beta _1 X_i + \varepsilon _i]$ \pause
\vfill
$Var[Y_i]= \sigma ^2$

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Formulação do modelo de regressão}

5) Pressupõe-se que os erros $\varepsilon_i$ são independentes (não-correlacionados). Se quaisquer $\varepsilon _i$ e $\varepsilon _j$ são independentes, então $Y_i$ e $Y_j$ também são independentes:
\vfill
\textbf{Em resumo:} Um modelo de regressão linear simples pressupõe que a resposta $Y_i$ vem de uma distribuição de probabilidade com média $E[Y_i]$ e variância $\sigma ^2$ constante para todos os níveis de X, e que quaisquer $Y_i$ e $Y_j$ são independentes. 

\end{frame}
%===============================================================================%


\section{Os Parâmetros da Regressão}


%===============================================================================%
\begin{frame}{Parâmetros do modelo de regressão}

Os parâmetros ou coeficientes da regressão $E[Y] = \beta _0 + \beta _1 X$ possuem nomes e significados específicos:
\vfill
\begin{itemize}
  \only<2>{\item O parâmetro $\beta _0$ é chamado de ?}
  \only<3-9>{\item O parâmetro $\beta _0$ é chamado de \textbf{intercepto}}
  \begin{itemize}
   \only<4>{\item $\beta _0$ representa ?}
   \only<5-9>{\item $\beta _0$ representa $E[Y_i]$ quando $X = 0$}
   \end{itemize}
\vfill
     \only<6>{\item O parâmetro $\beta _1$ é chamado de ?}
     \only<7-9>{\item O parâmetro $\beta _1$ é chamado de \textbf{inclinação (\emph{slope})} da reta}
  \begin{itemize}
      \only<8>{\item $\beta _1$ representa ?}
      \only<9>{\item $\beta _1$ representa o aumento em  $E[Y_i]$ para um aumento unitário em $X$}
   \end{itemize}
\end{itemize}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Parâmetros do modelo de regressão}
\centering
$E[Y] = 12 + 3X$
\vfill
<<param1,echo=FALSE,fig.height=6,fig.width=7.5,out.width='0.7\\linewidth'>>=
set.seed(1979)
x <- c(1:10)
y <- 12 + 3*x + rnorm(10,0,4)
plot(x,y,xlim=c(0,10),ylim=c(0,45),pch=19)
abline(a=12,b=3,lwd=2)
@


\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Parâmetros do modelo de regressão}
\centering
$E[Y] = 12 + 3X$, $\beta _0 = ?$
\vfill
<<param2,echo=FALSE,fig.height=6,fig.width=7.5,out.width='0.7\\linewidth'>>=
plot(x,y,xlim=c(0,10),ylim=c(0,45),pch=19)
abline(a=12,b=3,lwd=2)
@


\end{frame}
%===============================================================================%



%===============================================================================%
\begin{frame}{Parâmetros do modelo de regressão}
\centering
$E[Y] = 12 + 3X$, \alert{$\beta _0 = 12$}
\vfill
<<param3,echo=FALSE,fig.height=6,fig.width=7.5,out.width='0.7\\linewidth'>>=
plot(x,y,xlim=c(0,10),ylim=c(0,45),pch=19)
abline(a=12,b=3,lwd=2)
abline(v=0,lty=2,col='gray50')
segments(x0=0,y0=0,x1=0,y1=12,lwd=3,col="red")
points(0,12,pch=19,col="red")
@


\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Parâmetros do modelo de regressão}
\centering
$E[Y] = 12 + 3X$, $\beta _1 = ?$
\vfill
<<param4,echo=FALSE,fig.height=6,fig.width=7.5,out.width='0.7\\linewidth'>>=
plot(x,y,xlim=c(0,10),ylim=c(0,45),pch=19)
abline(a=12,b=3,lwd=2)
@


\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Parâmetros do modelo de regressão}
\centering
$E[Y] = 12 + 3X$, \alert{$\beta _1 = 3$}
\vfill
$X=5$, $Y=27$; $X=6$, $Y=30$; $30 - 27 = 3$
\vfill
<<param5,echo=FALSE,fig.height=6,fig.width=7.5,out.width='0.7\\linewidth'>>=
plot(x,y,xlim=c(0,10),ylim=c(0,45),pch=19)
abline(a=12,b=3,lwd=2)
abline(v=c(5,6),lty=2,col='gray50')
abline(h=c(27,30),lty=2,col='gray50')
segments(x0=5,y0=27,x1=6,y1=27,lwd=3,col="red")
segments(x0=6,y0=27,x1=6,y1=30,lwd=3,col="red")
points(5,27,pch=19,col="red")
points(6,30,pch=19,col="red")
@


\end{frame}
%===============================================================================%

\section{Estimando o Modelo}




%===============================================================================%
\begin{frame}{Estimando o modelo de regressão}

Assim como outras estatísticas, assume-se que o modelo $E[Y] = \beta_0 + \beta _1 X + \varepsilon$ corresponde a uma população.\pause
\vfill
Ao tomarmos uma amostra de valores de $X$ e $Y$, queremos estimar o modelo $\hat Y = b_0 + b_1X + e$ \pause
\vfill 
Idealmente, gostaríamos de usar um método onde $\hat Y$, $b_0$, $b_1$ e $e$ sejam bons estimadores (não-tendenciosos) de $Y$, $\beta _0$, $\beta _1$ e $\varepsilon$. 

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Estimando o modelo de regressão}

Para isso, podemos usar o método dos \textbf{Mínimos Quadrados Comuns (\emph{Ordinary Least Squares, OLS})}. Este método considera as diferenças entre cada valor $Y_i$ e seu valor esperado $E[Y_i]$:
\vfill
\begin{equation*}
Y_i - E[Y_i] = Y_i - (\beta _0 + \beta_1 X_i) \pause
\end{equation}
\vfill
Como no caso das variâncias, estamos interessados no quadrado destas diferenças, para que elas não se cancelem: \pause
\vfill
\begin{equation*}
Q = \sum_{i=1}^{n}(Y_i - (\beta _0 + \beta_1 X_i))^2
\end{equation*}
\vfill
\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Estimando o modelo de regressão}

De acordo com a formulação do método OLS, os melhores estimadores de $ \beta _0$ e $\beta _1$ são os valores $b_0$ e $b_1$ que minimizam o critério Q para as amostras obtidas. \pause
\vfill
\begin{columns}[c]

\column{0.5\linewidth}

\includegraphics[width=1\linewidth]{Fig_[3].jpg} \pause

\column{0.5\linewidth}

\includegraphics[width=1\linewidth]{Fig_[4].jpg}

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Estimando o modelo de regressão}

Os estimadores $b_0$ e $b_1$ que satisfazem o critério de mínimos quadrados podem ser determinados de duas maneiras: \pause
\vfill
\begin{itemize}
  \item Numericamente, através de procedimentos de busca  computacional.
  \vfill
  \item Analiticamente. Este método só funciona para Modelos Lineares Gerais. 
\end{itemize}
\vfill \pause
Para a regressão simples, o método analítico nos dá:
\vfill
\begin{columns}[c]
\column{0.5\linewidth}
\begin{equation*}
b_0 = \bar Y - b_1 \bar X
\end{equation*}
\column{0.5\linewidth}
\begin{equation*}
b_1 = \frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}
\end{equation*}  
\end{columns}  
\vfill
\tiny{Ver Kutner et al. (2005) Applied Linear Statistical Models. 5th ed. McGraw Hill. para a dedução analítica. I know you want to.}

\end{frame}
%===============================================================================%



%===============================================================================%
\begin{frame}{Exemplo}

\begin{columns}[t]

\column{0.5\linewidth}

<<exampleb1,echo=FALSE>>=
set.seed(1979)
X <- runif(15,0,10)
Y <- 2.2 + 1.3*X + rnorm(15,0,2.5)
X_Xbar2 <- (X - mean(X))^2
XY_XYbar <- (X - mean(X))*(Y - mean(Y))
xy.df <- data.frame(cbind(X,Y,XY_XYbar,X_Xbar2))
xy.mean <- data.frame(Xbar=mean(X),Ybar=mean(Y))
@
\begin{tiny}
\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
  $X$ & $Y$ & $(X - \bar X)(Y - \bar Y)$ & $(X - \bar X)^2$ \\ 
  \hline
  9.09 & 12.10 & 14.97 & 24.14 \\ 
  4.82 & 7.28 & -1.14 & 0.42 \\ 
  2.53 & 7.25 & 2.98 & 2.71 \\ 
  7.28 & 12.76 & 11.51 & 9.66 \\ 
  1.66 & 8.92 & 0.35 & 6.35 \\ 
  2.64 & 8.30 & 1.16 & 2.35 \\ 
  6.11 & 11.22 & 4.17 & 3.74 \\ 
  3.88 & 12.96 & -1.16 & 0.09 \\ 
  4.79 & 9.71 & 0.41 & 0.38 \\ 
  3.66 & 10.34 & -0.66 & 0.26 \\ 
  3.47 & 9.44 & -0.27 & 0.49 \\ 
  0.70 & 1.97 & 24.65 & 12.12 \\ 
  9.12 & 15.51 & 31.94 & 24.49 \\ 
  2.76 & 5.56 & 4.93 & 1.99 \\ 
  0.12 & 2.52 & 26.52 & 16.46 \\ 
   \hline
  $\bar X$ & $\bar Y$ & $\sum(X - \bar X)(Y - \bar Y)$ & $\sum(X - \bar X)^2$\\
  \hline
  4.18 & 9.06 & 120.35 &  105.66 \\
  \hline
\end{tabular}
\end{table}
\end{tiny}

\column{0.5\linewidth}

\begin{equation*}
b_1 = \frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}
\end{equation*}  \pause
\vfill
\begin{equation*}
b_1 = \frac{120.35}{105.66}
\end{equation*}  \pause
\vfill
\begin{equation*}
b_1 = 1.14
\end{equation*} 
\vfill
\end{columns}  


\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Exemplo}

\begin{columns}[c]

\column{0.5\linewidth}

\begin{tiny}
\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
  $X$ & $Y$ & $(X - \bar X)(Y - \bar Y)$ & $(X - \bar X)^2$ \\ 
  \hline
  9.09 & 12.10 & 14.97 & 24.14 \\ 
  4.82 & 7.28 & -1.14 & 0.42 \\ 
  2.53 & 7.25 & 2.98 & 2.71 \\ 
  7.28 & 12.76 & 11.51 & 9.66 \\ 
  1.66 & 8.92 & 0.35 & 6.35 \\ 
  2.64 & 8.30 & 1.16 & 2.35 \\ 
  6.11 & 11.22 & 4.17 & 3.74 \\ 
  3.88 & 12.96 & -1.16 & 0.09 \\ 
  4.79 & 9.71 & 0.41 & 0.38 \\ 
  3.66 & 10.34 & -0.66 & 0.26 \\ 
  3.47 & 9.44 & -0.27 & 0.49 \\ 
  0.70 & 1.97 & 24.65 & 12.12 \\ 
  9.12 & 15.51 & 31.94 & 24.49 \\ 
  2.76 & 5.56 & 4.93 & 1.99 \\ 
  0.12 & 2.52 & 26.52 & 16.46 \\ 
   \hline
  $\bar X$ & $\bar Y$ & $\sum(X - \bar X)(Y - \bar Y)$ & $\sum(X - \bar X)^2$\\
  \hline
  4.18 & 9.06 & 120.35 &  105.66 \\
  \hline
\end{tabular}
\end{table}
\end{tiny}

\column{0.5\linewidth}

\begin{equation*}
b_0 = \bar Y - b_1 \bar X
\end{equation*} \pause
\vfill
\begin{equation*}
b_0 = 9.06 - 1.14 \times 4.18
\end{equation*}  \pause
\vfill
\begin{equation*}
b_0 = 9.06 - 4.77
\end{equation*}  \pause 
\vfill
\begin{equation*}
b_0 = 4.3
\end{equation*} 
\vfill
\end{columns}  


\end{frame}
%===============================================================================%



%===============================================================================%
\begin{frame}
\centering
$ \hat Y = 4.3 + 1.14X$
\vfill
<<plotexample1,echo=FALSE,fig.height=4,fig.width=5,out.width='0.8\\linewidth'>>=
plot(X,Y,xlim=c(0,10),ylim=c(0,16))
@

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}
\centering
$ \hat Y = 4.3 + 1.14X$
\vfill
<<plotexample2,echo=FALSE,fig.height=4,fig.width=5,out.width='0.8\\linewidth'>>=
plot(X,Y,xlim=c(0,10),ylim=c(0,16))
abline(a=4.3,b=1.14,lwd=2,col='red')
@

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Os resíduos}

Olhando a nossa equação estimada, será que está faltando alguma coisa?
\vfill
\begin{equation*}
\hat Y_i = b_0 + b_1 X_i 
\end{equation} \pause
\vfill
Onde está o termo de erro estimado, $e$? 
\vfill
\begin{equation*}
\hat Y_i = b_0 + b_1 X_i + e
\end{equation*}
\vfill
\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Os resíduos}

Os erros estimados $e_i$ são chamados de \textbf{resíduos} da regressão:
\vfill
\begin{equation*}
\alert{e_i} = Y_i - \textcolor{blue}{\hat Y_i} 
\end{equation} 
\vfill
\centering
<<residplot,echo=FALSE,fig.height=4,fig.width=5,out.width='0.6\\linewidth'>>=
M <- lm(Y ~ X)
Ypred <- predict(M)
par(mar=c(5.1,4.1,1,2.1))
plot(X,Y,xlim=c(0,10),ylim=c(0,16),pch=19)
abline(M,col='blue',lwd=1.5)
segments(x0=X, x1=X,y0=Ypred,y1=Y,col='red',lwd=2)
points(X,Ypred,pch=19,col='blue')
@
\vfill
\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Propriedades do modelo de regressão}

\begin{enumerate}
  \item A soma dos resíduos é zero : $\sum e_i = 0$ \pause
  \vfill
  \item A soma dos quadrados dos resíduos, $\sum e_i^2$ é um mínimo \pause
  \vfill
  \item A soma dos valores observados $Y_i$ é igual a soma dos valores ajustados $\hat Y_i$ \pause
  \vfill
  \item A soma dos resíduos ponderada pelos valores de $X_i$ é zero: $\sum X_i e_i = 0$ \pause
  \vfill
  \item Devido a 1) e 4), a soma dos resíduos ponderada pelos valores de $\hat Y_i$ também é zero: \sum \hat Y_i e_i = 0$ \pause
  \vfill
  \item \textbf{A reta de regressão sempre passa pelo ponto ($\bar X$,$\bar Y$)}
\end{enumerate}

\end{frame}
%===============================================================================%

\section{Estimando a Variância do Modelo}


%===============================================================================%
\begin{frame}{Estimando a variância de uma população}

\begin{small}
A variância de $\varepsilon$ também precisa ser estimada, a fim de caracterizar a distribuição de probabilidade de $Y$ para cada nível de $X$, e permitir inferências sobre o modelo. \pause
\vfill
\textbf{Relembrando:} A variância de uma população $Y$ ($\sigma ^2$) pode ser estimada pela variância de uma amostra ($s^2$), através da \textbf{soma dos quadrados} dos desvios de $Yi$ a partir de $\bar Y$:
\vfill
\begin{equation*}
\sum_{i-1}^{n}(Y_i - \bar Y)^2
\end{equation*}
\vfill \pause
Para obter $s^2$, nós dividimos a soma dos quadrados pelos graus de liberdade associados com essa soma:
\begin{equation*}
s^2 =\frac{\sum_{i-1}^{n}(Y_i - \bar Y)^2}{n-1}
\end{equation*}
\end{small}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Estimando a variância do modelo}

\begin{small}
Uma das pressuposições do modelo de regressão linear é que $s^2$ é constante. A única diferença para a variância comum é que, no modelo, a distribuição de $Y_i$ varia de acordo com o nível de $X$, então os desvios são calculados em relação a $\hat Y$, e não $\bar Y$: \pause
\vfill
\begin{equation*}
Y_i - \hat Y = e_i
\end{equation*}
\vfill \pause
E a soma do quadrado destes valores é chamada de \textbf{Soma dos Quadrados dos Erros/Resíduos} ($SQ_{res}$\footnote{\tiny{Essa nomenclatura varia, vejam exatamente quem é quem ao ler um livro/artigo}}):
\begin{equation*}
SQ_{res} =\sum_{i-1}^{n}(Y_i - \hat Y)^2 = \sum_{i-1}^{n}e_i^2
\end{equation*}
\end{small}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Estimando a variância do modelo}

\begin{small}
A $SQ_{res}$ tem $n-2$ graus de liberdade, por que dois graus são perdidos estimando-se $\beta _0$ e $\beta_1$. Assim, temos que $s^2$ é:
\vfill 
\begin{equation*}
s^2 = MQ_{res} = \frac{SQ_{res}}{n-2} = \frac{\sum(Y_i - \hat Y)^2}{n-2} = \frac{\sum e_i^2}{n-2}
\end{equation*}
\end{small}
\vfill
A divisão por $n - 2$ é apenas uma normalização para proporção. Mas não é necessária para entender a quantidade de variação. 
\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}[fragile]

\begin{columns}[t]

\column{0.5\linewidth}

<<esd3,echo=TRUE,size='tiny'>>=
set.seed(1979)
x <- runif(15,1,10)
y <- 2 + 3*x + rnorm(15,0,3)
m <- lm(y ~ x)
summary(m)
@

\column{0.5\linewidth}

<<esd3plot,echo=-2,fig.height=3.8,fig.width=5,out.width='1\\linewidth',size='tiny'>>=
ypred <- predict(m)
par(mar=c(5.1, 4.1, 0, 2.1))
plot(x,y,xlim=c(0,10),ylim=c(0,40))
abline(m)
segments(x0=x, x1=x,y0=ypred,y1=y,
         col='red',lwd=2)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}[fragile]

\begin{columns}[t]

\column{0.5\linewidth}

<<esd10,echo=TRUE,size='tiny'>>=
set.seed(1979)
x <- runif(15,1,10)
y <- 2 + 3*x + rnorm(15,0,10)
m <- lm(y ~ x)
summary(m)
@

\column{0.5\linewidth}

<<esd10plot,echo=-2,fig.height=3.8,fig.width=5,out.width='1\\linewidth',size='tiny'>>=
ypred <- predict(m)
par(mar=c(5.1, 4.1, 0, 2.1))
plot(x,y,xlim=c(0,10),ylim=c(0,40))
abline(m)
segments(x0=x, x1=x,y0=ypred,y1=y,
         col='red',lwd=2)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Partição da variância e o Coeficiente de Determinação}

Agora, já sabemos como estimar todos os componentes do modelo:  $\hat Y$, $b_0$, $b_1$, e $s^2$. O que mais precisamos? \pause
\vfill
\begin{itemize}
  \item De um método para avaliar a qualidade da estimação \pause
  \vfill
  \only<3>{\item De um método para avaliar o ajuste do modelo}
  \end{itemize}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Partição da variância e o Coeficiente de Determinação}

\begin{itemize}
\item Como vimos antes, $SQ_{res}$ nos dá a variancia dos resíduos. \pause
\vfill
\item A variância contida em $SQ_{res}$ é a quantidade de variação que é aleatória, e não pôde ser capturada pelo modelo \pause
\vfill
\item Essa variação é uma parte da variância total de $Y$ (Soma dos Quadrados Totais, $SQ_{tot}$) \pause
\vfill 
\item Podemos então definir a "variância explicada pelo modelo", como sendo a diferença entre a variância total de $Y$ ($SQ_{tot}$), e a variância dos resíduos ($SQ_{res}$):
\end{itemize}
\vfill
\begin{equation*}
  SQ_{reg} =  SQ_{tot} - SQ_{res}
\end{equation*}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Partição da variância e o Coeficiente de Determinação}

Essa partição pode ser melhor entendida ao se considerarem duas situações extremas: \pause
\vfill

\begin{itemize}
  \item Se todos os valores de $Y$ caíssem exatamente em cima da reta, $SQ_{res} = 0$, e $SQ_{reg} = SQ_{tot}$ \pause
  \vfill
  \item Se não há relação entre $X$ e $Y$, $\beta _1 = 0$, e $Y = \beta_0 + \varepsilon$. Nesse caso, $Y \sim N(\beta _0,\sigma)$, e $SQ_{tot} = SQ_{res}$  \pause
\end{itemize}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}[fragile]

\begin{columns}[c]

\column{0.5\linewidth}

<<sse0,echo=TRUE,size='tiny',warning=FALSE>>=
set.seed(1979)
x <- runif(15,1,10)
y <- 2 + 3*x
m <- lm(y ~ x)
anova(m)["Sum Sq"]
var(y)*(15-1)          
@

<<sse0plot,echo=F,fig.height=3.8,fig.width=5,out.width='0.7\\linewidth',size='tiny'>>=
par(mar=c(3,3,0,0))
plot(x,y,xlim=c(0,10),ylim=c(0,40))
abline(m)
@


\column{0.5\linewidth}

<<ssr0,echo=TRUE,size='tiny'>>=
set.seed(1979)
x <- runif(15,1,10)
y <- 2 + rnorm(15,0,3)
m <- lm(y ~ x)
anova(m)["Sum Sq"]
var(y)*(15-1)
@

<<ssr0plot,echo=F,fig.height=3.8,fig.width=5,out.width='0.7\\linewidth',size='tiny'>>=
par(mar=c(3,3,0,0))
plot(x,y,xlim=c(0,10),ylim=c(0,40))
abline(m)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Coeficiente de Determinação}

\begin{columns}[c]

\column{0.3\linewidth}

\includegraphics[width=0.9\linewidth]{heman.jpg}

\column{0.8\linewidth}

A partir desta formulação, chegamos ao \textbf{Coeficiente de Determinação ($R^2$)}: 
\vfill
\begin{equation*}
R^2 = \frac{SQ_{reg}}{SQ_{tot}} = \frac{SQ_{reg}}{SQ_{reg}+SQ_{res}}
\end{equation*} \pause
\vfill
Como podemos interpretar o valor de $R^2$? \pause
\vfill

$R^2$ nos dá a proporção da variância total de $Y$ explicada pelo modelo de regressão

\bigskip

\scriptsize{* $r^2$ se refere a um modelo simples, e $R^2$ a um modelo multivariado.}

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Em resumo}
\setbeamercovered{transparent}

\begin{itemize}

\item Um modelo de \textbf{regressão linear} estima uma relação estatística entre \textbf{$X$} e \textbf{$Y$}, através de coeficientes lineares \textbf{$\beta$} \pause
\vfill
\item Esta relação é caracterizada por uma \textbf{co-variação} entre os níveis de $X$ e $E[Y]$ \pause
\vfill
\item A existência de co-variação não implica em \textbf{causalidade} \pause
\vfill
\item A variância de $Y$ não capturada pelo modelo constitui o \textbf{erro da regressão} ($\varepsilon$) \pause
\vfill
\item A \textbf{variância de} \textbf{$Y_i$} a cada nível de $X$ é dada pela variância de $\varepsilon$ \pause
\vfill
\item A \textbf{variância total} de \textbf{$Y$} é dada pela variância de $\varepsilon$ + a relação $\beta _0 + \beta _1 X$ 
\vfill
\end{itemize}
\vfill
\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Em resumo}
\setbeamercovered{transparent}

\begin{itemize}

\item Os coeficientes $\beta _0$ e $\beta _1$ determinam o \textbf{intercepto} e a \textbf{inclinação da reta} \pause
\vfill
\item A reta de regressão ($\hat Y = b_0 + b_1X + e$) é estimada pelo método de \textbf{mínimos quadrados}, que busca minizar Soma dos Quadrados dos Erros ($SQ_{res}$) \pause
\vfill
\item A partir da diferença entre $SQ_{res}$ e a Soma dos Quadrados Totais ($SQ_{tot} = Var[Y]$), podemos estimar a Soma dos Quadrados da Regressão ($SQ_{reg}$) \pause
\vfill
\item A relação $\frac{SQ_{reg}}{SQ_{tot}}$ é chamada de $R^2$, e nos diz a proporção da variância total de $Y$ explicada pelo modelo$
\vfill
\end{itemize}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}[fragile]

Interpretando o output do modelo de regressão no R \dots até agora.

<<size = 'tiny',echo=-c(1:7)>>=
set.seed(1979)
x <- runif(20,0,20)
y <- 1.5 + 3.2*x + rnorm(20,0,5)

m <- lm(y ~ x)
options(width=100)
summary(m)
@

\end{frame}
%===============================================================================%

\end{document}
