\documentclass{beamer}
\usefonttheme[onlymath]{serif}

\usepackage[portuguese]{babel}
\usepackage{graphicx}
\usepackage{ulem} % Para texto em strikeout
\usepackage{amsmath}
\usepackage{amssymb}

\usetheme{m}

\ifdefined\knitrout 
\renewenvironment{knitrout}{\setlength{\topsep}{0mm}}{}
\else
\fi

\title{Aula 6: Modelos Lineares Gerais III}
\subtitle{Análise Estatística e Modelagem de Dados Ecológicos}
\author{\textbf{Thiago S. F. Silva} - tsfsilva@rc.unesp.br}
\institute{Programa de Pós Graduação em Ecologia e Biodiversidade - UNESP}
\date{\today}

\graphicspath{{C:/Users/thiago/OneDrive/UNESP/Pos_graduacao/Eco/2015/Estatistica_2015/Aulas/Aula_6_diagnostico_remediacao/figuras/}}

\begin{document}

<<opts, echo=FALSE>>=
options(width=80)
library(ggplot2)
plotheme <- theme(axis.line = element_line(colour = "black", size=1), panel.grid.major = element_blank(),panel.grid.minor = element_blank(), panel.border = element_blank(),panel.background = element_blank()) 
@

%===============================================================================%
\begin{frame}[plain] % plain avoids a badbox error from page number in title page
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}
%===============================================================================%

\section{Diagnóstico, Remediação e Validação}

%===============================================================================%
\begin{frame}{Diagnóstico, Remediação e Validação}

Nossos modelos lineares são baseados em uma série de pressuposições, a lembrar:
\vfill
\begin{enumerate}
  \item Os valores/níveis de $X$ são medidos sem erro \pause
  \vfill
  \item Existe uma relação linear entre $X$ e $Y$ (só para regressão) \pause
  \vfill
  \item Os erros $\varepsilon$ (e por consequência, $Y$) tem variância constante ($\sigma^2$) \pause
  \vfill
  \item Os erros $\varepsilon$ (e por consequência, $Y$) são independentes \pause
  \vfill
  \item Os erros $\varepsilon$ (e por consequência, $Y$) são normalmente distribuídos:
  \vfill
  \begin{itemize}
    \item $\varepsilon \sim N(0,\sigma^2)$
    \vfill
    \item  $Y\sim N(\beta _0 + \beta _1 X,\sigma^2)$
  \end{itemize}
\end{enumerate}


\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Diagnóstico e Remediação}

Muitas vezes, estas pressuposições não correspondem à realidade. Por esta razão, todo processo de modelagem inclui etapas de diagnóstico, remediação, e validação: \pause
\vfill
\textbf{Diagnóstico}: processo de avaliação da adequação dos dados e resultados às pressuposições do modelo. \pause
\vfill
\textbf{Remediação}: processo de melhoria da adequação dos dados e resultados às pressuposições do modelo. \pause
\vfill
\textbf{Validação}: processo de verificação da performance do modelo na explicação/previsão do fenômeno de interesse

\end{frame}
%===============================================================================%


\section{Análise de resíduos}


% %===============================================================================%
% \begin{frame}{Análise de resíduos}
% 
% A análise descritiva e gráfica dos resíduos do modelo pode diagnosticar a maioria das violações às pressuposições do modelo.
% \vfill
% Em alguns casos, pode ser útil normalizar o valor dos resíduos: \pause
% \vfill
% \textbf{Resíduos Normalizados:} Valores originais normalizados pela variância. \pause
% \vfill
% \textbf{Resíduos Estudantizados:} (\emph{studentized residuals}) Valores originais divididos pela variância dos resíduos da mesma regressão, com o valor em questão excluído. Tem esse nome pois seguem uma distribuição $t$.\pause
% 
% \end{frame}
% %===============================================================================%

%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[t]

\column{0.6\linewidth}
\small

Uma das princpais análises diagnósticas é um scatterplot dos resíduos vs.  valores estimados ($\hat Y$) \pause

\bigskip

Vejamos um exemplo de um modelo de regressão apropriado: 
\begin{itemize}
\item Resíduos aletoriamente distribuidos ao redor de zero 
\item Variação constante ao longo de $Y_i$
\end{itemize}

\column{0.4\linewidth}

<<reg1,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
set.seed(1979)
x <- runif(30,0,15)
set.seed(1979)
y <- 2 + 1.2*x + rnorm(30,0,2)
m <- lm(y~x)
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(predict(m),residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]),mgp=c(2.5,1,0))
abline(h=0,col='red')
@
\end{columns}


\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small

Uma das princpais análises diagnósticas é um scatterplot dos resíduos vs.  valores estimados ($\hat Y$) 

\bigskip

Vejamos um exemplo de um modelo de regressão apropriado:
\begin{itemize}
\item Resíduos aletoriamente distribuidos ao redor de zero
\item Variação constante ao longo de $\hat{Y}_i$
\end{itemize}

\column{0.4\linewidth}

<<r2,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,,col='red')
boxplot(residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]))
@
\end{columns}


\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small

Dados com resíduos não-normais: 
\begin{itemize}
\item Resíduos positivos maiores do que resíduos negativos 
\item Distribuição Assimétrica 
\end{itemize}

\column{0.4\linewidth}

<<r3,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
set.seed(3423)
y <- 2.4 + 1.7*x +  rf(30,4,15)
m <- lm(y~x)
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(predict(m),residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]))
abline(h=0,col='red')
@
\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small

Dados com resíduos não-normais: 
\begin{itemize}
\item Resíduos positivos maiores do que resíduos negativos 
\item Distribuição Assimétrica 
\end{itemize}

\column{0.4\linewidth}

<<r4,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
boxplot(residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]))
@
\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small

Resíduos não-normais: \textbf{Q-Q plot}
\begin{itemize}
\item Ferramenta gráfica bastante utilizada para avaliação de aderência à normalidade \pause
\item Plota os quantis dos dados contra os quantis correspondentes de uma distribuição normal com os mesmos parâmetros: $N(0, s^2)$ \pause
\item Quanto mais normal a distribuição, mais ``iguais'' serão os quantis \pause
\end{itemize}

\column{0.4\linewidth}

<<r5,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
set.seed(1576)
y1 <- rnorm(100,0,3)
set.seed(3343)
y2 <- rf(100,4,15)
par(mar=c(5,5,0,0))
qqnorm(y1,xlim=c(-10,10),ylim=c(-10,10),main=NA)
qqline(y1)
qqnorm(y2,xlim=c(-5,5),ylim=c(-5,5),main=NA)
qqline(y2)
@
\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}

\small{Dados com resíduos heteroscedásticos:} \pause

\begin{itemize}
\begin{scriptsize}


\item \textbf{homoscedástico} = variância constante

\item \textbf{heteroscedástico} = variância inconstante \pause

\end{scriptsize}

\item Resíduos aleatoriamente distribuídos ao redor de zero 

\item Variância dos resíduos aumenta (ou diminui) ao longo de $\hat Y$ 

\end{itemize}

\column{0.4\linewidth}

<<r6,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
set.seed(3423)
y <- 2.4 + 1.7*x + rnorm(30,0,1)*0.5*x
m <- lm(y~x)
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(predict(m),residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]))
abline(h=0,col='red')
@
\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small{Dados com resíduos heteroscedásticos:} \pause

\begin{itemize}

\begin{scriptsize}

\item \textbf{homoscedástico} = variância constante

\item \textbf{heteroscedástico} = variância inconstante 

\end{scriptsize}

\item Resíduos aleatoriamente distribuídos ao redor de zero 

\item Variância dos resíduos aumenta (ou diminui) ao longo de $\hat Y$ 

\end{itemize}

\column{0.4\linewidth}

<<r7,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
boxplot(residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]))
@
\end{columns}

\end{frame}
%============================================================================%




%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small

Dados com resíduos não-independentes:

\bigskip
\textbf{Curva de Keeling }\pause

Concentração de CO2 atmosférico medido em Mauna Loa, Hawaii
\bigskip
\begin{itemize}
\item Resíduos distribuidos sistematicamente ao redor de zero
\end{itemize}

\column{0.4\linewidth}

<<r8,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
keeling <- read.csv('keeling.csv')
m <- lm(media ~ data.dec, keeling[-c(1:550),])
plot(interp ~ data.dec, keeling[-c(1:550),])
abline(m,col='red')
plot(predict(m),residuals(m))
abline(h=0,col='red')

@
\end{columns}

\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small

Dados com resíduos não-independentes:

\bigskip
\textbf{Curva de Keeling }

Concentração de CO2 atmosférico medido em Mauna Loa, Hawaii
\bigskip
\begin{itemize}
\item Resíduos distribuídos sistematicamente ao redor de zero
\end{itemize}


\column{0.4\linewidth}

<<r9,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
plot(interp ~ data.dec, keeling[-c(1:550),])
abline(m,col='red')
plot(predict(m),residuals(m),,type='l')
abline(h=0,col='red')

@
\end{columns}

\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small


Dados com resíduos não-independentes:

\bigskip
\textbf{Curva de Keeling }

Concetração de CO2 atmosférico medido em Mauna Loa, Hawaii
\bigskip
\begin{itemize}
\item Resíduos distribuidos sistematicamente ao redor de zero
\end{itemize}
\column{0.4\linewidth}

<<r10,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
plot(interp ~ data.dec, keeling[-c(1:550),])
abline(m,col='red')
boxplot(residuals(m))
@
\end{columns}

\end{frame}
%============================================================================%


%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small


\textbf{Função de Autocorrelação}

\bigskip
Plota a correlação entre $X$ e seus próprios valores, com diferentes lags.

\column{0.4\linewidth}

<<r101,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
plot(interp ~ data.dec, keeling[-c(1:550),])
abline(m,col='red')
acf(residuals(m),lag.max=6)
@
\end{columns}

\end{frame}
%============================================================================%



%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small


Relação entre $X$ e $Y$ não é linear \pause

\bigskip
\begin{itemize}
\item Resíduos distribuidos segundo um padrão
\item O padrão sugere o tipo de relação
\end{itemize}

\column{0.4\linewidth}

<<r11,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
set.seed(3423)
y <- 2.4 + 1.7*x + 0.5*(x^2)+ rnorm(30,0,5)
m <- lm(y~x)
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(predict(m),residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]))
abline(h=0,col='red')
@
\end{columns}

\end{frame}
%============================================================================%


%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small


Relação entre $X$ e $Y$ não é linear

\bigskip
\begin{itemize}
\item Resíduos distribuidos segundo um padrão
\item O padrão sugere o tipo de relação
\end{itemize}

\column{0.4\linewidth}

<<r12,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
set.seed(3423)
y <- 2.4 + 30*log(x) + rnorm(30,0,5)
m <- lm(y~x)
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(predict(m),residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]))
abline(h=0,col='red')
@
\end{columns}

\end{frame}
%============================================================================%



%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small


Ausência de uma variável explicativa

\bigskip
\begin{itemize}
\item Grande parte da variância não explicada por $X_1$ 
\item Resíduos nem sempre revelam um padrão
\item Mas pode ocorrer forte relação entre os resíduos e a varável omitida
\end{itemize}

\column{0.4\linewidth}

<<r13,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
set.seed(94)
x2 <- runif(30,0,20)
set.seed(3423)
y <- 2.4 + 0.9*x + 2*x2 + rnorm(30,0,5)
m <- lm(y~x)
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(predict(m),residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]))
abline(h=0,col='red')
@
\end{columns}

\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[c]

\column{0.6\linewidth}
\small


Ausência de uma variável explicativa

\bigskip
\begin{itemize}
\item Grande parte da variância não explicada por $X_1$ 
\item Resíduos nem sempre revelam um padrão
\item Mas pode ocorrer forte relação entre os resíduos e a varável omitida
\end{itemize}

\column{0.4\linewidth}

<<r14,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(x2,residuals(m),xlab=expression(X[2]),ylab=expression(e[i]))
@
\end{columns}

\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{Tipos de Resíduos}
\footnotesize

\textbf{Resíduos Brutos (\emph{Raw})}: Os resíduos originais do modelo, na escala de valores original de $Y$. Se oriundos de um modelo múltiplo, podem assumir valores "estranhos".
\vfill
\textbf{Resíduos Normalizados (\emph{Standardized})}: Resíduos originais, divididos pelo erro padrão dos resíduos: $e_i / \sqrt{MQ_{Res}}$. Contudo, apesar da pressuposição de variância constante dos \textbf{erros ($\mathbf{\varepsilon}$)}, os resíduos tendem a ser menos confiáveis quanto mais distante de $(\bar{X},\bar{Y})$.  \pause
\vfill
\textbf{Resíduos Estudantizados (\emph{Studentized})}: Normalização dos resíduos que leva em conta esse efeito de distância de $\bar{X}$, através da fórmula $e_i / MQ_{Res} \times \sqrt{(1 - h_{ii})}$. $h_{ii}$ é um elemento da matriz desenho do modelo, que captura esse efeito.

\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[t]

\column{0.6\linewidth}
\small

\begin{itemize}
\item \textbf{Resíduos Brutos} 
\end{itemize}

\column{0.4\linewidth}

<<rawres,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
set.seed(1979)
x <- runif(30,0,15)
set.seed(1979)
y <- 2 + 1.2*x + rnorm(30,0,2)
y[30] <- y[30*1.2]
m <- lm(y~x)
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(predict(m),residuals(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]),mgp=c(2.5,1,0))
abline(h=0,col='red')
@
\end{columns}


\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[t]

\column{0.6\linewidth}
\small

\begin{itemize}
\item Resíduos Brutos
\item \textbf{Resíduos Normalizados}
\end{itemize}

\column{0.4\linewidth}

<<stdres,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(predict(m),rstandard(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]),mgp=c(2.5,1,0))
abline(h=0,col='red')
@
\end{columns}


\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Análise de resíduos}

\begin{columns}[t]

\column{0.6\linewidth}
\small

\begin{itemize}
\item Resíduos Brutos
\item Resíduos Normalizados
\item \textbf{Resíduos Estudantizados}
\end{itemize}

\column{0.4\linewidth}

<<studres,fig.height=3,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,0,0))
plot(y ~ x)
abline(m,col='red')
plot(predict(m),rstudent(m),xlab=expression(hat(Y)[i]),ylab=expression(e[i]),mgp=c(2.5,1,0))
abline(h=0,col='red')
@
\end{columns}


\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Pontos Influentes}

Outro diagnóstico importante é a avaliação do efeito de observações isoladas sobre o ajuste final do modelo linear: \pause
\vfill
\textbf{Leverage}: Mede o efeito de valores extremos de $X$. \emph{Leverage} vem de \emph{lever} (alavanca). Valores extremos de $X$ podem alavancar a reta de regressão, que se ``equilibra'' em $(\bar{X},\bar{Y})$ 
\pause
\vfill
\textbf{Distância:} Mede o efeito de valores extremos de $Y$ (resíduos extremos). \pause
\vfill
\textbf{Influência:} Combinação de distância e \emph{leverage}, captura efeito total de um \emph{outlier} sobre a reta de regressão

\end{frame}
%============================================================================%


%===============================================================================%
\begin{frame}{Pontos Influentes}

É possível ter um ponto com \emph{leverage} alto, e influência baixa? \pause
\vfill
\begin{columns}[c]

\column{0.5\linewidth}
<<lev,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
x <- c(1,3,4,5,7,8,9,12,14,18,30)
set.seed(34)
y <- 2.4 + 1.4*x + c(rnorm(10,0,2),25)
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@
\pause
\column{0.5\linewidth}
<<levp,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
x <- c(1,3,4,5,7,8,9,12,14,18,30)
set.seed(34)
y <- 2.4 + 1.4*x + c(rnorm(11,0,2))
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@
\pause
\end{columns}
\vfill
Sim, se a \emph{distância} for baixa.
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{Pontos Influentes}

É possível ter um ponto com \emph{distância} alta, e influência baixa? \pause
\vfill
\begin{columns}[c]

\column{0.5\linewidth}
<<di1,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
x <- c(1,3,4,5,7,8,9,12,14,18,20,23,28)
set.seed(34)
y <- 2.4 + 1.4*x + rnorm(13,0,2)
y[11] <- y[11]*2
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[-11]~x[-11]),col='black')
@
\pause
\column{0.5\linewidth}
<<d12,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
set.seed(34)
y <- 2.4 + 1.4*x + rnorm(13,0,2)
y[8] <- y[8]*2
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[8],y[8],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[-8]~x[-8]),col='black')
@
\pause
\end{columns}
\vfill
Sim, se o \emph{leverage} for baixo.

\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{Pontos Influentes}
\small

Como identificar pontos influentes? \pause
\vfill
\textbf{DFFITS}: Diferença normalizada entre o valor de $\hat Y_i$ no modelo completo, e o valor da mesma estimativa no modelo onde o ponto $X_i,Y_i$ é removido, $\hat Y_{i(-i)}$. Identifica pontos com influência sobre estimativas de $Y$ isoladas. \pause
\vfill
\textbf{Distância de Cook}: Similar a DFFITS, mas ao invés de avaliar a diferença em um único ponto, avalia a soma dos quadrados das diferenças de todos os $\hat Y$. Identifica pontos com influência sobre todas as estimativas de $Y$. \pause
\vfill
\textbf{DFBETAS}: Diferença normalizada entre os valores dos $b$ no modelo completo, e o valor da mesma estimativa no modelo onde o ponto $X_i,Y_i$ é removido, $b_{i(-i)}$. Identifica pontos com influência sobre a inclinação da reta.

\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf0,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
x <- c(1,3,4,5,7,8,9,12,14,18,30)
set.seed(34)
y <- 2.4 + 1.4*x + c(rnorm(10,0,2),25)
m <- lm(y~x)
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@
\pause
\column{0.5\linewidth}
<<inf00,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
res <- residuals(m)
par(mar=c(4,4,1,0))
plot(predict(m),res,xlab="Y_hat",ylab="Resíduos")
points(predict(m)[11],res[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%


%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf000,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf0000,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
res <- rstandard(m)
par(mar=c(4,4,1,0))
plot(predict(m),res,xlab="Y_hat",ylab="Resíduos")
points(predict(m)[11],res[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf0-0,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf00-0,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
res <- rstudent(m)
par(mar=c(4,4,1,0))
plot(predict(m),res,xlab="Y_hat",ylab="Resíduos")
points(predict(m)[11],res[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf2,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- dffits(m)
par(mar=c(4,4,1,0))
plot(fits,xlab="Observação",ylab="DFFITS",ylim=c(-1,13))
points(11,fits[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%



%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf3,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf4,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- cooks.distance(m)
par(mar=c(4,4,1,0))
plot(fits,xlab="Observação",ylab="Cook's distance",ylim=c(-1,13))
points(11,fits[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf5,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf6,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- dfbeta(m)
par(mar=c(4,4,1,0))
plot(1:11,fits[,1],xlab="Observação",ylab="DFBETAS - Intercepto",ylim=c(-5,2))
points(11,fits[11,1],pch=19,col="red")
abline(h=0,col='red')
@
\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf7,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf8,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- dfbeta(m)
par(mar=c(4,4,1,0))
plot(1:11,fits[,2],xlab="Observação",ylab="DFBETAS - Slope",ylim=c(-0.2,0.7))
points(11,fits[11,2],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf99,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
x <- c(1,3,4,5,7,8,9,12,14,18,30)
set.seed(34)
y <- 2.4 + 1.4*x + c(rnorm(11,0,2))
m <- lm(y~x)
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@
\pause
\column{0.5\linewidth}
<<inf109,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
res <- residuals(m)
par(mar=c(4,4,1,0))
plot(predict(m),res,xlab="Y_hat",ylab="Resíduos")
points(predict(m)[11],res[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf90,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf100,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
res <- rstandard(m)
par(mar=c(4,4,1,0))
plot(predict(m),res,xlab="Y_hat",ylab="Resíduos")
points(predict(m)[11],res[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf91,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf101,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
res <- rstudent(m)
par(mar=c(4,4,1,0))
plot(predict(m),res,xlab="Y_hat",ylab="Resíduos")
points(predict(m)[11],res[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf93,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf10,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- dffits(m)
par(mar=c(4,4,1,0))
plot(fits,xlab="Observação",ylab="DFFITS",ylim=c(-1,13))
points(11,fits[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf11,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf12,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- cooks.distance(m)
par(mar=c(4,4,1,0))
plot(fits,xlab="Observação",ylab="Cook's distance",ylim=c(-1,13))
points(11,fits[11],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf13,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf14,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- dfbeta(m)
par(mar=c(4,4,1,0))
plot(1:11,fits[,1],xlab="Observação",ylab="DFBETAS - Intercepto",ylim=c(-5,2))
points(11,fits[11,1],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf15,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[11],y[11],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf16,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- dfbeta(m)
par(mar=c(4,4,1,0))
plot(1:11,fits[,2],xlab="Observação",ylab="DFBETAS - Slope",ylim=c(-0.2,0.7))
points(11,fits[11,2],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%



%===============================================================================%
\begin{frame}{$\downarrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf17,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
x <- c(1,3,4,5,7,8,9,12,14,18,20,23,28)
set.seed(34)
y <- 2.4 + 1.4*x + rnorm(13,0,2)
y[8] <- y[8]*2
m <- lm(y~x)
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[8],y[8],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[-8]~x[-8]),col='black')
@
\pause
\column{0.5\linewidth}
<<inf200,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
res <- residuals(m)
par(mar=c(4,4,1,0))
plot(predict(m),res,xlab="Y_hat",ylab="Resíduos")
points(predict(m)[8],res[8],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf201,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[8],y[8],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf202,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
res <- rstandard(m)
par(mar=c(4,4,1,0))
plot(predict(m),res,xlab="Y_hat",ylab="Resíduos")
points(predict(m)[8],res[8],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf203,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[8],y[8],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf204,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
res <- rstudent(m)
par(mar=c(4,4,1,0))
plot(predict(m),res,xlab="Y_hat",ylab="Resíduos")
points(predict(m)[8],res[8],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\uparrow$ Leverage e $\downarrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf9,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[8],y[8],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[1:10]~x[1:10]),col='black')
@

\column{0.5\linewidth}
<<inf103,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- dffits(m)
par(mar=c(4,4,1,0))
plot(fits,xlab="Observação",ylab="DFFITS",ylim=c(-1,13))
points(8,fits[8],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\downarrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf19,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[8],y[8],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[-8]~x[-8]),col='black')
@

\column{0.5\linewidth}
<<inf20,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- cooks.distance(m)
par(mar=c(4,4,1,0))
plot(fits,xlab="Observação",ylab="Cook's distance",ylim=c(-1,12))
points(8,fits[8],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\downarrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf21,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[8],y[8],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[-8]~x[-8]),col='black')
@

\column{0.5\linewidth}
<<inf22,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- dfbeta(m)
par(mar=c(4,4,1,0))
plot(1:13,fits[,1],xlab="Observação",ylab="DFBETAS - Intercepto",ylim=c(-5,2))
points(8,fits[8,1],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%

%===============================================================================%
\begin{frame}{$\downarrow$ Leverage e $\uparrow$ Distance}

\begin{columns}[c]

\column{0.5\linewidth}
<<inf23,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=FALSE>>=
par(mar=c(4,4,1,0))
plot(y ~ x)
points(x[8],y[8],col='red',pch=19)
abline(lm(y~x),col='red')
abline(lm(y[-8]~x[-8]),col='black')
@

\column{0.5\linewidth}
<<inf24,fig.height=4,fig.width=4,out.width='1\\linewidth',echo=1,size='tiny'>>=
fits <- dfbeta(m)
par(mar=c(4,4,1,0))
plot(1:13,fits[,2],xlab="Observação",ylab="DFBETAS - Slope",ylim=c(-0.1,0.7))
points(8,fits[8,2],pch=19,col="red")
abline(h=0,col='red')
@

\end{columns}
\end{frame}
%============================================================================%


%===============================================================================%
\begin{frame}{$\downarrow$ Leverage e $\uparrow$ Distance}

Visualização + diagnóstico!

\begin{columns}[c]

\column{0.5\linewidth}
<<inf25,fig.height=4,fig.width=5,out.width='1\\linewidth',echo=FALSE>>=
df.f <- data.frame(cbind(x,y,residuals(m),predict(m),dffits(m),cooks.distance(m),dfbetas(m)))
names(df.f)<-c("x","y","resid","pred","DFFITS","Cooks","DFBETAS.Int","DFBETAS.Slope")
library(ggplot2)
ggplot(df.f,aes(pred,resid)) + geom_point(aes(color=DFFITS,size=Cooks)) + theme_bw(base_size = 12) + geom_hline(y = 0,color='red')
@

\column{0.55\linewidth}
<<inf26,fig.height=4,fig.width=5.5,out.width='1\\linewidth',echo=F,size='tiny'>>=
ggplot(df.f,aes(pred,resid)) + geom_point(aes(color=DFBETAS.Int,size=DFBETAS.Slope)) + theme_bw(base_size = 12) + geom_hline(y = 0,color='red')
@

\end{columns}
\end{frame}
%============================================================================%


%===============================================================================%
\begin{frame}{\emph{Testes Estatísticos para Pressuposições}}

Os modelos lineares gerais são robustos, e podem tolerar pequenos desvios. Mas se voce realmente quer testar...\pause
\vfill
\begin{itemize}
\item \textbf{Normalidade:} Kolmogorov-Smirnov, Shapiro-Wilk, Lilliefors
\vfill
\item \textbf{Heteroscedasticidade:} Breusch-Pagan, White
\vfill
\item \textbf{Independência:} Durbin-Watson, Função de Autocorrelação
\vfill
\end{itemize}
\end{frame}
%===============================================================================%

\section{Remediação}


%===============================================================================%
\begin{frame}{Remediação}

Após a análise diagnóstica, descobrimos que nosso modelo viola uma ou mais pressuposições. O que fazer? \pause
\vfill
\begin{itemize}
  \item Transformação de variáveis \pause
  \vfill
  \item Métodos Robustos e/ou Não-Paramétricos (outra aula)\pause
  \vfill
  \item Outros modelos que não Modelos Lineares Gerais (outra aula)\pause
  \vfill
  \item Métodos de aleatorização e reamostragem 
\end{itemize}
  
\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Transformação de Variáveis}

A alternativa mais simples para violações dos pressupostos é a transformação de variáveis \pause
\vfill
Mas\ldots se transformarmos as variáveis originais, não vamos alterar a relação entre elas? \pause
\vfill
As transformações devem ser \textbf{monotônicas} (preservam a ordem relativa dos dados): Se $ X_i > X_j$, então $f(X_i) > f(X_j)$, e vice versa. \pause
\vfill
Se usarmos funções monotônicas, podemos alterar a distância relativa entre os pontos, e assim a variância e a forma da distribuição 
  
\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Funções de potência}

A família de funções de potência oferece flexibilidade, dentro de uma mesma especificação: \pause
\vfill
$Y' = cY^\lambda$ inclui: \pause
\vfill
\begin{itemize}
\item $Y^{-\lambda} = \dfrac{1}{Y^\lambda}$ \pause, se $\lambda = 1$, $Y' = Y^{-1} = 1/Y$ \pause
\vfill
\item $Y^{\frac{1}{\lambda}} = \sqrt[\lambda]{Y}$ \pause
\vfill
\item $Y^\lambda$ \pause
\end{itemize}
\vfill
$c$ é apenas uma constante de escala
  
\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Funções de potência}

As relações de potência podem também ser expressas na forma abaixo, conhecida como transformação de Box-Cox \pause
\vfill
$ Y' = \frac{Y^\lambda - 1}{\lambda}$, para $\lambda \neq 0$  \pause
\vfill
$Y' = log(Y)$, para $\lambda = 0$ \pause
\vfill
A expressão acima é válida pois $\lim_{\lambda \to 0} \frac{Y^\lambda - 1}{\lambda} = \log _e (X)$ \pause
\vfill
Normalmente se prefere $\log _{10}$ para facilitar a interpretação, pois um aumento de 1 em $\log _{10}(Y)$ é o mesmo que multiplicar $Y$ por 10
  
\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}


\centering

\includegraphics[width=0.8\linewidth]{TukeyBulge.png}


\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem1,fig.height=4,fig.width=4,out.width='0.7\\linewidth',echo=FALSE>>=
x <- c(1:20)
set.seed(89)
y <- 2 + x^3 
plot(x,y)
@

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem2,fig.height=4,fig.width=4,out.width='0.7\\linewidth',size='scriptsize',echo=TRUE>>=
plot(x,sqrt(y))
@

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem3,fig.height=4,fig.width=4,out.width='0.7\\linewidth',size='scriptsize',echo=TRUE>>=
plot(x,log10(y))
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem4,fig.height=4,fig.width=4,out.width='0.7\\linewidth',size='scriptsize',echo=TRUE>>=
plot(x^2,y)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem5,fig.height=4,fig.width=4,out.width='0.7\\linewidth',size='scriptsize',echo=TRUE>>=
plot(x^3,y)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem6,fig.height=4,fig.width=4,out.width='0.7\\linewidth',echo=FALSE>>=
x <- c(1:20)
set.seed(89)
y <- 2 + x^0.002 
plot(x,y)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem7,fig.height=4,fig.width=4,out.width='0.7\\linewidth',echo=FALSE>>=
plot(sqrt(x),y)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem8,fig.height=4,fig.width=4,out.width='0.7\\linewidth',echo=FALSE>>=
plot(log10(x),y)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem9,fig.height=4,fig.width=4,out.width='0.7\\linewidth',echo=FALSE>>=
plot(x,y^2)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Regra da Convexidade de Tukey}

\begin{columns}[c]

\column{0.5\linewidth}
\centering
\includegraphics[width=1.3\linewidth]{TukeyBulge.png}

\column{0.8\linewidth}
\centering
<<rem20,fig.height=4,fig.width=4,out.width='0.7\\linewidth',echo=FALSE>>=
plot(x,y^3)
@

\end{columns}

\end{frame}
%===============================================================================%



%===============================================================================%
\begin{frame}[fragile]{Otimização de $\lambda$}

 Podemos utilizar métodos computacionais para encontrar o melhor valor de $\lambda$ (médodo de máxima verossimilhança, \emph(maximum likelihood)

\begin{columns}[c]

\column{0.5\linewidth}

<<Otim,size='tiny',fig.keep='none'>>=
x <- c(1:30)
y <- 2 + x^2.786 
m <-lm(y~x)
plot(x,y)
abline(m)
@

\column{0.5\linewidth}

\centering
<<oti2,fig.height=4,fig.width=4,out.width='0.9\\linewidth',size='scriptsize',echo=FALSE>>=
plot(x,y)
abline(m)
@

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}[fragile]{Otimização de $\lambda$}

 Podemos utilizar métodos computacionais para encontrar o melhor valor de $\lambda$ (médodo de máxima verossimilhança, ou \emph{maximum likelihood})

\begin{columns}[c]

\column{0.5\linewidth}

<<oti3,size='tiny',fig.keep='none'>>=
library(MASS)
lambda <- boxcox(m)
@


\column{0.5\linewidth}
\centering
<<oti4,fig.height=4,fig.width=4,out.width='0.9\\linewidth',size='scriptsize',echo=FALSE>>=
lambda <- boxcox(m)
@

\end{columns}

\end{frame}
%===============================================================================%
% 
% 
%===============================================================================%
\begin{frame}[fragile]{Otimização de $\lambda$}

 Podemos utilizar métodos computacionais para encontrar o melhor valor de $\lambda$ (médodo de máxima verossimilhança, \emph(maximum likelihood)

\begin{columns}[c]

\column{0.5\linewidth}

<<oti5,size='tiny',fig.keep='none'>>=
which(lambda$y == max(lambda$y))
lambda$x[59]
newy <- (y^0.3434343 -1)/0.3434343
plot(x,newy)
@


\column{0.5\linewidth}
\centering

<<oti6,fig.height=4,fig.width=4,out.width='0.9\\linewidth',size='scriptsize',echo=F>>=
plot(x,newy)
@

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Transformações: normalidade e variância}

 O uso de transformações não se limita à linearização de variáveis, mas também é de grande ajuda na aproximação dos dados para uma distribuição normal e variância constante
\begin{columns}[c]

\column{0.5\linewidth}

<<t1,fig.height=4,fig.width=4,out.width='0.9\\linewidth',size='scriptsize',echo=F>>=
x <- c(1:30)
set.seed(3423)
y <- 30.4 + 1.7*x +  rnorm(30,4,3)*x
plot(x,y)
m <- lm(y~x)
abline(m)
# plot(predict(m),residuals(m))
# abline(h=0,col='red')
# plot(x,log(y))
# mlog <-lm(log(y)~x)
# plot(predict(mlog),residuals(mlog))
# abline(h=0,col='red')
@


\column{0.5\linewidth}
\centering

<<t2,fig.height=4,fig.width=4,out.width='0.9\\linewidth',size='scriptsize',echo=F>>=
# x <- c(1:30)
# set.seed(3423)
# y <- 30.4 + 1.7*x +  rnorm(30,4,3)*x
#plot(x,y)
#m <- lm(y~x)
#abline(m)
plot(predict(m),residuals(m))
abline(h=0,col='red')
#plot(x,log(y))
# mlog <-lm(log(y)~x)
# plot(predict(mlog),residuals(mlog))
# abline(h=0,col='red')
@

\end{columns}

\end{frame}
%===============================================================================%



%===============================================================================%
\begin{frame}{Transformações: normalidade e variância}

 O uso de transformações não se limita à linearização de variáveis, mas também é de grande ajuda na aproximação dos dados para uma distribuição normal e variância constante
\begin{columns}[c]

\column{0.5\linewidth}

<<t3,fig.height=4,fig.width=4,out.width='0.9\\linewidth',size='scriptsize',echo=F>>=
# x <- c(1:30)
# set.seed(3423)
# y <- 30.4 + 1.7*x +  rnorm(30,4,3)*x
# plot(x,y)
# m <- lm(y~x)
# abline(m)
# plot(predict(m),residuals(m))
# abline(h=0,col='red')
plot(x,log(y))
mlog <-lm(log(y)~x)
abline(mlog)
# plot(predict(mlog),residuals(mlog))
# abline(h=0,col='red')
@


\column{0.5\linewidth}
\centering

<<t4,fig.height=4,fig.width=4,out.width='0.9\\linewidth',size='scriptsize',echo=F>>=
# x <- c(1:30)
# set.seed(3423)
# y <- 30.4 + 1.7*x +  rnorm(30,4,3)*x
#plot(x,y)
#m <- lm(y~x)
#abline(m)
#plot(predict(m),residuals(m))
#abline(h=0,col='red')
# plot(x,log(y))
# mlog <-lm(log(y)~x)
plot(predict(mlog),residuals(mlog))
abline(h=0,col='red')
@

\end{columns}

\end{frame}
%===============================================================================%

\section{Validação}


%===============================================================================%
\begin{frame}{Validação}

Já fizemos a análise exploratória, ajustamos o modelo, analisamos os resíduos, resolvemos problemas de violação das pressuposições. Nosso modelo está pronto. \pause
\vfill
E agora?\pause
\vfill
A última etapa do processo de modelagem consiste na \textbf{validação}, isto é, avaliação da ``veracidade'' do modelo \pause
\vfill
Se o modelo é explicativo, queremos ter certeza sobre nossos coeficientes \pause
\vfill
Se o modelo é preditivo, queremos ter certeza sobre as novas predições 

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{Validação}

Os coeficientes do nosso modelo explicam a relação entre $X$ e $Y$, e através desta relação podemos prever novos valores de $Y$ \pause
\vfill
Mas o quanto podemos confiar em $b_0$ e $b_1$ como estimativas de $\beta _0$ e $\beta _1$, e nos valores de $\hat Y_{i(novo)}$ ? \pause
\vfill
1) Intervalos de confiança paramétricos \pause
\vfill
2) Validação independente \pause
\vfill
3) Métodos de aleatorização e reamostragem

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}[fragile]{Exemplo}

 Distância de frenagem pode ser predita pela velocidade do veículo?

\begin{columns}[c]

\column{0.6\linewidth}

<<v1,size='tiny', fig.keep='none',echo=-(1:4)>>=
data(cars)
cars$dist <- cars$dist*0.3048
cars$speed <- cars$speed*1.60934
m0 <- lm(dist ~ speed, cars)
summary(m0)
@

\column{0.4\linewidth}

<<v2,fig.height=4,fig.width=4,out.width='1\\linewidth',size='scriptsize',echo=F>>=
plot (dist ~ speed, cars,xlab="Velocidade(km/h)", ylab="Distância(m)")
abline(m0)
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Validação Cruzada com Dados Independentes}

A melhor medida da capacidade de predição do modelo é a sua performance em estimar valores não usados no ajuste \pause
\vfill
Mas ao mesmo tempo, queremos usar o máximo de observações possíveis, para ter o melhor ajuste \pause

\vfill
\textbf{Validação cruzada:} dividimos a amostra em 2 partes iguais, e usamos cada metade para validar um modelo ajustado à outra metade

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}[fragile]{Exemplo}

 Distância de frenagem pode ser predita pela velocidade do veículo?

\begin{columns}[c]

\column{0.6\linewidth}

<<v3,size='tiny', fig.keep='none',echo=T>>=
dim(cars)
set.seed(89)
samp <- sample(1:50,25,rep=F)
cars1 <- cars[samp,]
cars2 <- cars[-samp,]
m1 <- lm(dist ~ speed, cars1)
m2 <- lm(dist ~ speed, cars2)
@

\column{0.4\linewidth}

<<v4,fig.height=4,fig.width=4,out.width='.7\\linewidth',size='scriptsize',echo=F>>=
par=(mar=c(5,5,0,0))
plot (dist ~ speed, cars1,xlab="Velocidade(km/h)", ylab="Distância(m)")
abline(m1)
plot (dist ~ speed, cars2,xlab="Velocidade(km/h)", ylab="Distância(m)")
abline(m2)
@

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}[fragile]{Exemplo}

 Distância de frenagem pode ser predita pela velocidade do veículo?

\begin{columns}[c]

\column{0.6\linewidth}

<<v5,size='tiny', fig.keep='none',echo=T>>=
summary(m1)$coefficients
summary(m2)$coefficients

@

\column{0.4\linewidth}

<<v6,fig.height=4,fig.width=4,out.width='.7\\linewidth',size='scriptsize',echo=F>>=
par=(mar=c(5,5,0,0))
plot (dist ~ speed, cars1,xlab="Velocidade(km/h)", ylab="Distância(m)")
abline(m1)
plot (dist ~ speed, cars2,xlab="Velocidade(km/h)", ylab="Distância(m)")
abline(m2)
@

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}[fragile]{Exemplo}

 Distância de frenagem pode ser predita pela velocidade do veículo?

\begin{columns}[c]

\column{0.6\linewidth}

<<v7,size='tiny', fig.keep='none',echo=T>>=
pr1 <- predict(m1,cars2)
pr2 <-predict(m2,cars1)
rmse1 <- sqrt(mean((cars2$dist - pr1)^2))
rmse2 <- sqrt(mean((cars1$dist - pr2)^2))
rmse1
rmse2
@

\column{0.4\linewidth}

<<v8,fig.height=4,fig.width=4,out.width='.7\\linewidth',size='scriptsize',echo=F>>=
par=(mar=c(5,5,0,0))
plot (pr1 ~ cars2$dist,xlab="Observado", ylab="Estimado")
plot (pr2 ~ cars1$dist,xlab="Observado", ylab="Estimado")
@

\end{columns}

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{\emph{Jackknife} ou \emph{LOOCV}}

Não seria ótimo se pudéssemos usar o máximo possível de observações pra estimar o erro? \pause
\vfill
E se, ao invés de dividir meio a meio, deixássemos uma observação de fora, e repetíssemos $n$ vezes? \pause

\vfill
\textbf{\emph{Jacknife} ou \emph{Leave One Out Cross-Validation (LOOCV)}} 

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}[fragile]{Exemplo}

 Distância de frenagem pode ser predita pela velocidade do veículo?

\begin{columns}[c]

\column{0.6\linewidth}

<<v9,size='tiny', fig.keep='none',echo=T>>=
b0 <- vector()
b1 <- vector()
rq <- vector()
for (i in c(1:50)){
  m <- lm(dist ~ speed, data=cars[-i,])
  b0 <- c(b0,coefficients(m)[1])
  b1 <- c(b1,coefficients(m)[2])
  rq <- c(rq,summary(m)$r.squared)
}
@

\column{0.4\linewidth}

<<v10,fig.height=4,fig.width=4,out.width='.7\\linewidth',size='scriptsize',echo=F>>=
par=(mar=c(5,5,0,0))
hist(b0)
hist(b1)
@

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}[fragile]{Exemplo}

 Distância de frenagem pode ser predita pela velocidade do veículo?

\begin{columns}[c]

\column{0.6\linewidth}

<<v11,size='tiny', fig.keep='none',echo=T>>=
pred <- b0 + cars$speed * b1
erro <- cars$dist-pred
rmse <- sqrt(mean(erro^2))
rmse
@

\column{0.4\linewidth}

<<v12,fig.height=4,fig.width=4,out.width='.7\\linewidth',size='scriptsize',echo=F>>=
par=(mar=c(5,5,0,0))
plot (pred ~ cars$dist,xlab="Observado", ylab="Estimado")
hist(erro)
@

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{\emph{Jackknife} ou \emph{LOOCV}}

Esse método pode também ser usado para criar intervalos de confiança para $\beta _0$, $\beta _1$, etc. \pause
\vfill
Conduza a aleatorização, e reporte os percentis $\alpha/2$ e $1-\alpha/2$ \pause

\vfill

\textbf{Problema:} poucas observações

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}{\emph{Bootstrap}}

\textbf{Bootstrap}

Generalização dos métodos de reamostragem \pause
\vfill
Selecione $n$ novas amostras, com reposição, e recalcule o modelo. Repita \textbf{muitas} vezes. \pause

\vfill

Observações mais frequentes vão ser reamostradas mais vezes \pause

\vfill

O resultado final aproxima a distribuição original de $\varepsilon$ e $Y$ (seja ela qual for)

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}[fragile]{Exemplo}

 Distância de frenagem pode ser predita pela velocidade do veículo?

\begin{columns}[c]

\column{0.6\linewidth}

<<v13,size='tiny', fig.keep='none',echo=T>>=
b0 <- vector()
b1 <- vector()
rq <- vector()
for (i in c(1:1000)){
  sub <- sample(1:50,50,replace=T)
  m <- lm(dist ~ speed, data=cars[sub,])
  b0 <- c(b0,coefficients(m)[1])
  b1 <- c(b1,coefficients(m)[2])
  rq <- c(rq,summary(m)$r.squared)
}
@

\column{0.4\linewidth}

<<v14,fig.height=4,fig.width=4,out.width='.7\\linewidth',size='scriptsize',echo=F>>=
par=(mar=c(5,5,0,0))
hist(b0)
hist(b1)
@

\end{columns}

\end{frame}
%===============================================================================%


%===============================================================================%
\begin{frame}[fragile]{Exemplo}

<<cis,size='tiny', fig.keep='none',echo=T>>=

quantile(b0,probs=c(0.025,0.975))

quantile(b1,probs=c(0.025,0.975))

confint(m0)
@

\end{frame}
%===============================================================================%

%===============================================================================%
\begin{frame}{Conclusão}

É importante garantir que os dados satisfaçam às pressuposições dos Modelos Lineares Gerais\pause
\vfill
Mas ao mesmo tempo, vale lembrar que estes métodos são bastante robustos, especialmente quando $n$ é grande \pause
\vfill
Diagnóstico e remediação, mas sem obsessão! \pause
\vfill

\end{frame}
%===============================================================================%

\end{document}